{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "collapsed_sections": [
        "Q3M7woqMfzB-"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decoding: From Logits to the Speculative Decoding and Guard Rails\n",
        "\n",
        "We've covered everything from loss to logits - and now we're going to dive in to how we select the next token in details, and all the options we can use to do it!\n",
        "\n",
        "Breakout Room #1: Logits to Tokens\n",
        "- Task 1: Dependencies\n",
        "- Task 2: Generating Tokens!\n",
        "  - ğŸ—ï¸ Activity #1:\n",
        "- Task 3: Data Preprocessing\n",
        "  - â“Question #1\n",
        "- Task 4: Alternate Decoding Examples:\n",
        "  - ğŸ‘ªâ“ Discussion Question #1\n",
        "\n",
        "Breakout Room #2: Speculative Decoding and Guard Rails\n",
        "- Task 5: Speculative Decoding\n",
        "  - â“ Discussion Question #2\n",
        "- Task 6: Guard Rails\n",
        "  - ğŸ‘ªâ“ Discussion Question #2"
      ],
      "metadata": {
        "id": "oBRtW67UDRiW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Dependencies\n",
        "\n",
        "Today we'll be using a classic minamalist implementation of a decoder-only transformer model called `nanoGPT`, built by the one-and-only Andrej Karpathy - found [here](https://github.com/karpathy/nanoGPT/tree/master)!\n",
        "\n",
        "It does require a few dependencies - though most are covered by the default Colab environment.\n",
        "\n",
        "> NOTE: You will need to make sure you're in a GPU enabled environment for effective use of this notebook."
      ],
      "metadata": {
        "id": "Lu7EnLapFJYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU datasets tiktoken wandb tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj3ShLeSEvH3",
        "outputId": "f9ce369a-97e5-486d-fc0b-38ee1f1119ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBdaBurtA8NP",
        "outputId": "e11df9df-5d13-47d6-cdee-ecb54e45fa02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nanoGPT'...\n",
            "remote: Enumerating objects: 682, done.\u001b[K\n",
            "remote: Total 682 (delta 0), reused 0 (delta 0), pack-reused 682 (from 1)\u001b[K\n",
            "Receiving objects: 100% (682/682), 952.47 KiB | 7.16 MiB/s, done.\n",
            "Resolving deltas: 100% (385/385), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/karpathy/nanoGPT.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd nanoGPT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xjcjW0zEhbM",
        "outputId": "0750241c-8725-4894-c3b3-dc8772b3cbe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nanoGPT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Generating Tokens!\n",
        "\n",
        "Let's just try to do some inference and see what happens before we dig in."
      ],
      "metadata": {
        "id": "6ZtIRopJGfRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSKkqMHdErid",
        "outputId": "46fe7bd2-f58d-49eb-d47e-d7382334b982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "2024-12-03 22:18:23.240879: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-03 22:18:23.256615: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 22:18:23.277649: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 22:18:23.284031: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 22:18:23.298951: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 22:18:24.445190: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "config.json: 100% 689/689 [00:00<00:00, 4.37MB/s]\n",
            "model.safetensors: 100% 6.43G/6.43G [00:30<00:00, 211MB/s]\n",
            "generation_config.json: 100% 124/124 [00:00<00:00, 900kB/s]\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "One possibility is that they have a universe-wide perspective that allows for no contradictions. And, if so, then why is there contradiction?\n",
            "\n",
            "Another possibility is that the universe is one big joke. They are not the only joke in the universe. But it is not an empty universe.\n",
            "\n",
            "Hence the problems with the universe's Big Bang theory, which tells us that the universe started out in a singularity with a zero initial mass. The Big Bang is essentially consistent with\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You'll notice that we pass *in* text - and we receive *back* text from our model."
      ],
      "metadata": {
        "id": "i9VQDtfcGych"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ğŸ—ï¸ Activity #1:\n",
        "\n",
        "*Where* (in the architecture) exactly does NanoGPT go \"from logits to predicted token\"? Provide a screenshot from [this visualization](https://bbycroft.net/llm)!"
      ],
      "metadata": {
        "id": "0MhDBLg9sU5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: How Does the LLM Generate Tokens\n",
        "\n",
        "So, we pas in text - and get text back - but how do we actually generate each token?\n",
        "\n",
        "You might have heard the term \"auto-regressive\" or \"causal\" kicking around when reading about LLMs - and what those terms, in a simplified sense, mean is straightfoward enough:\n",
        "\n",
        "- They take an input, and generate a single token\n",
        "- They append that token to the input and repeat this process for as long as we want it to repeat (or use heuristics to determine when to stop, such as when we see a stop token)\n",
        "\n",
        "Let's take a look at the function that does this in the `nanoGPT` repository.\n",
        "\n"
      ],
      "metadata": {
        "id": "NKNzRbwuG0M2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "@torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n",
        "```"
      ],
      "metadata": {
        "id": "2Jpj1ytjH5--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is a Logit?!\n",
        "\n",
        "Technically - a logit is a \"raw unnormalized score\".\n",
        "\n",
        "However, we can think of them as scores for each token in our vocabulary. These scores aren't probabilities in and of themselves - but they can be easily converted to probabilities through the softmax function."
      ],
      "metadata": {
        "id": "tvDfy32YKnxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is Temperature Doing?!\n",
        "\n",
        "While something like `top_k` makes intuitive sense - what in the heck is temperature doing here?\n",
        "\n",
        "In order to understand - let's look at a few examples!\n",
        "\n",
        "Starting with an easy `temperature = 1.0`.\n",
        "\n",
        "> NOTE: We'll also define our softmax function!"
      ],
      "metadata": {
        "id": "sRvnaByHLUr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "    return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())"
      ],
      "metadata": {
        "id": "lOWCrlUsMSnJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "temperature = 1.0\n",
        "\n",
        "logits = np.array([6, 2, 7, 0.1, -8, 9])\n",
        "\n",
        "temp_scaled_logits = logits / temperature\n",
        "print(f\"Scaled Logits: {temp_scaled_logits}\")\n",
        "\n",
        "softmaxed_logits = softmax(temp_scaled_logits)\n",
        "print(f\"Softmax-ed Logits: {softmaxed_logits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXpso0sYLdwI",
        "outputId": "6c6022c0-252d-4d39-b1a0-4e41c352f70c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Logits: [ 6.   2.   7.   0.1 -8.   9. ]\n",
            "Softmax-ed Logits: [4.19729385e-02 7.68761185e-04 1.14094276e-01 1.14982549e-04\n",
            " 3.49017038e-08 8.43049007e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - our logits are not changed, and our softmax output has quite a bit of variety - from `e-08` to `e-1`, meaning that our index with the score `9` is most likely to be selected, but it's not absurdly likely.\n",
        "\n",
        "Let's look at an example with a very low temperature next!"
      ],
      "metadata": {
        "id": "AbE3L_VKMByY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 0.1\n",
        "\n",
        "logits = np.array([6, 2, 7, 0.1, -8, 9])\n",
        "\n",
        "temp_scaled_logits = logits / temperature\n",
        "print(f\"Scaled Logits: {temp_scaled_logits}\")\n",
        "\n",
        "softmaxed_logits = softmax(temp_scaled_logits)\n",
        "print(f\"Softmax-ed Logits: {softmaxed_logits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE5fO2SSM3Ct",
        "outputId": "697eb761-6324-4be4-cd53-d2f9408f3281"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Logits: [ 60.  20.  70.   1. -80.  90.]\n",
            "Softmax-ed Logits: [9.35762295e-14 3.97544973e-31 2.06115362e-09 2.22736356e-39\n",
            " 1.47889750e-74 9.99999998e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see - now that we changed our temperature to be very low - the index with score `9` is *vastly* more likely than any other option.\n",
        "\n",
        "This is the idea that a low (<1) temperature value will scale our logits to be larger - resulting in a sharper probability distribution after softmax.\n",
        "\n",
        "Let's look at a final example with a higher temperature."
      ],
      "metadata": {
        "id": "5VVVtg8_NQ06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = 100\n",
        "\n",
        "logits = np.array([6, 2, 7, 0.1, -8, 9])\n",
        "\n",
        "temp_scaled_logits = logits / temperature\n",
        "print(f\"Scaled Logits: {temp_scaled_logits}\")\n",
        "\n",
        "softmaxed_logits = softmax(temp_scaled_logits)\n",
        "print(f\"Softmax-ed Logits: {softmaxed_logits}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-tRO3UDNmt1",
        "outputId": "2dc7a01d-3f03-46bc-aa95-0b8947b5b86c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaled Logits: [ 0.06   0.02   0.07   0.001 -0.08   0.09 ]\n",
            "Softmax-ed Logits: [0.17201758 0.16527268 0.17374639 0.16216214 0.1495449  0.1772563 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --temperature=100.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYmie7meLtAG",
        "outputId": "d5a38418-8713-4757-bab7-b85c1a760a50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: temperature = 100.0\n",
            "2024-12-03 22:19:49.546142: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-03 22:19:49.563045: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 22:19:49.584012: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 22:19:49.590321: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 22:19:49.605187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 22:19:50.757143: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "What is the answer to life, the universe, and everything? Have any prominent clerics yet responded they have? Religion columnist Keith Nearer considers â€” though keeps religion contained<|endoftext|>Police charge second Maryland inmate is now under search! If news breaking only had an 8 but kept u Peking for few tehtsts instead WN!!(19 mins 2):/police-indiewatson â€¦ wwwâ€¦â€¦#blackmagic420â€¦â€¦.(63 minute tape.. 1 long teute from da aikkkâ€¦[hobby][play with[SIB1 10\n",
            "c'\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can see that, while our index with score `9` is still the most likely - we can see that the probabilities are much closer together!"
      ],
      "metadata": {
        "id": "CBMM9M1VM7We"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### â“ Question #1:\n",
        "\n",
        "Why is the softmax operation so important for decoding?"
      ],
      "metadata": {
        "id": "cZWakEUxtJlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Alternate Decoding Examples\n",
        "\n",
        "Let's look at a few other methods we could use to go from our logits to some text!\n",
        "\n",
        "We'll look at each component of the following script for each different method in detail as we go through them!"
      ],
      "metadata": {
        "id": "j6-dh6ebR250"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Modified Generation Script"
      ],
      "metadata": {
        "id": "Q3M7woqMfzB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_sample_script = r'''\n",
        "\"\"\"\n",
        "Sample from a trained model with various decoding strategies, including repetition penalty\n",
        "\"\"\"\n",
        "import os\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import tiktoken\n",
        "from model import GPTConfig, GPT\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "init_from = 'resume' # either 'resume' (from an out_dir) or a gpt2 variant (e.g. 'gpt2-xl')\n",
        "out_dir = 'out' # ignored if init_from is not 'resume'\n",
        "start = \"\\n\" # or \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
        "num_samples = 10 # number of samples to draw\n",
        "max_new_tokens = 500 # number of tokens generated in each sample\n",
        "temperature = 0.8 # 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
        "top_k = 200 # retain only the top_k most likely tokens, clamp others to have 0 probability\n",
        "top_p = 0.0 # If set, use nucleus sampling\n",
        "beam_width = 0 # If set, use beam search with this width\n",
        "greedy = False # use greedy decoding\n",
        "repetition_penalty = 1.0 # 1.0 means no penalty, > 1.0 discourages repetition, < 1.0 encourages repetition\n",
        "seed = 1337\n",
        "device = 'cuda' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n",
        "compile = False # use PyTorch 2.0 to compile the model to be faster\n",
        "exec(open('configurator.py').read()) # overrides from command line or config file\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
        "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
        "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
        "\n",
        "# model\n",
        "if init_from == 'resume':\n",
        "    # init from a model saved in a specific directory\n",
        "    ckpt_path = os.path.join(out_dir, 'ckpt.pt')\n",
        "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "    gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "    model.load_state_dict(state_dict)\n",
        "elif init_from.startswith('gpt2'):\n",
        "    # init from a given GPT-2 model\n",
        "    model = GPT.from_pretrained(init_from, dict(dropout=0.0))\n",
        "\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model) # requires PyTorch 2.0 (optional)\n",
        "\n",
        "# look for the meta pickle in case it is available in the dataset folder\n",
        "load_meta = False\n",
        "if init_from == 'resume' and 'config' in checkpoint and 'dataset' in checkpoint['config']: # older checkpoints might not have these...\n",
        "    meta_path = os.path.join('data', checkpoint['config']['dataset'], 'meta.pkl')\n",
        "    load_meta = os.path.exists(meta_path)\n",
        "if load_meta:\n",
        "    print(f\"Loading meta from {meta_path}...\")\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "    # TODO want to make this more general to arbitrary encoder/decoder schemes\n",
        "    stoi, itos = meta['stoi'], meta['itos']\n",
        "    encode = lambda s: [stoi[c] for c in s]\n",
        "    decode = lambda l: ''.join([itos[i] for i in l])\n",
        "else:\n",
        "    # ok let's assume gpt-2 encodings by default\n",
        "    print(\"No meta.pkl found, assuming GPT-2 encodings...\")\n",
        "    enc = tiktoken.get_encoding(\"gpt2\")\n",
        "    encode = lambda s: enc.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "    decode = lambda l: enc.decode(l)\n",
        "\n",
        "# Top-P (nucleus) sampling\n",
        "def top_p_sampling(logits, p):\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    sorted_indices_to_remove = cumulative_probs > p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "    probs = probs.masked_fill(indices_to_remove, 0.0)\n",
        "    return torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "# Beam Search with Repetition Penalty\n",
        "def beam_search(model, x, max_new_tokens, beam_width, repetition_penalty):\n",
        "    beams = [(x, 0)]\n",
        "    for _ in range(max_new_tokens):\n",
        "        candidates = []\n",
        "        for beam_idx, (sequence, log_prob) in enumerate(beams):\n",
        "            if sequence.size(1) > model.config.block_size:\n",
        "                idx_cond = sequence[:, -model.config.block_size:]\n",
        "            else:\n",
        "                idx_cond = sequence\n",
        "            logits, _ = model(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "\n",
        "            # Apply repetition penalty\n",
        "            if repetition_penalty != 1.0:\n",
        "              # Get unique tokens in the generated sequence\n",
        "              unique_tokens = torch.unique(x[0])\n",
        "\n",
        "              # Create a mask for the tokens that have been used\n",
        "              mask = torch.zeros_like(logits[0]).bool()\n",
        "              mask[unique_tokens] = True\n",
        "\n",
        "              # Apply the penalty only to the used tokens\n",
        "              logits[0, mask] /= repetition_penalty\n",
        "\n",
        "            probs = F.log_softmax(logits, dim=-1)\n",
        "            top_probs, top_indices = probs.topk(beam_width)\n",
        "            for prob, idx in zip(top_probs[0], top_indices[0]):\n",
        "                new_sequence = torch.cat((sequence, idx.unsqueeze(0).unsqueeze(0)), dim=1)\n",
        "                new_log_prob = log_prob + prob.item()\n",
        "                candidates.append((new_sequence, new_log_prob))\n",
        "        beams = sorted(candidates, key=lambda x: x[1], reverse=True)[:beam_width]\n",
        "    return beams[0][0]\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate(model, x, max_new_tokens, temperature=1.0, top_k=None, top_p=None, beam_width=None, repetition_penalty=1.0, greedy=False):\n",
        "    print(f\"Starting generation with max_new_tokens={max_new_tokens}\")\n",
        "\n",
        "    if beam_width != 0:\n",
        "        print(f\"Using beam search with width {beam_width}\")\n",
        "        return beam_search(model, x, max_new_tokens, beam_width, repetition_penalty)\n",
        "\n",
        "    for i in range(max_new_tokens):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Generated {i} tokens\")\n",
        "\n",
        "        if x.size(1) > model.config.block_size:\n",
        "            idx_cond = x[:, -model.config.block_size:]\n",
        "        else:\n",
        "            idx_cond = x\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # Apply repetition penalty\n",
        "        if repetition_penalty != 1.0:\n",
        "          # Get unique tokens in the generated sequence\n",
        "          unique_tokens = torch.unique(x[0])\n",
        "\n",
        "          # Create a mask for the tokens that have been used\n",
        "          mask = torch.zeros_like(logits[0]).bool()\n",
        "          mask[unique_tokens] = True\n",
        "\n",
        "          # Apply the penalty only to the used tokens\n",
        "          logits[0, mask] /= repetition_penalty\n",
        "\n",
        "        if greedy:\n",
        "            # Greedy decoding: select the token with the highest probability\n",
        "            idx_next = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
        "        else:\n",
        "            # Existing sampling methods\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            if top_p != 0.0:\n",
        "                idx_next = top_p_sampling(logits, top_p)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        x = torch.cat((x, idx_next), dim=1)\n",
        "\n",
        "    print(\"Generation complete\")\n",
        "    return x\n",
        "\n",
        "# encode the beginning of the prompt\n",
        "if start.startswith('FILE:'):\n",
        "    with open(start[5:], 'r', encoding='utf-8') as f:\n",
        "        start = f.read()\n",
        "start_ids = encode(start)\n",
        "x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "# run generation\n",
        "with torch.no_grad():\n",
        "    with ctx:\n",
        "        for k in range(num_samples):\n",
        "            print(f\"Generating sample {k+1}/{num_samples}\")\n",
        "            y = generate(model, x, max_new_tokens, temperature=temperature, top_k=top_k, top_p=top_p,\n",
        "                         beam_width=beam_width, repetition_penalty=repetition_penalty, greedy=greedy)\n",
        "            print(decode(y[0].tolist()))\n",
        "            print('---------------')\n",
        "'''\n",
        "\n",
        "with open('extended_sample.py', 'w') as f:\n",
        "  f.write(new_sample_script)"
      ],
      "metadata": {
        "id": "k4N5X2KDQ-4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vanilla Sample Generation\n",
        "\n",
        "```python\n",
        "@torch.no_grad()\n",
        "def generate(model, x, max_new_tokens, temperature=1.0, top_k=None, top_p=None, beam_width=None, repetition_penalty=1.0, greedy=False):\n",
        "    print(f\"Starting generation with max_new_tokens={max_new_tokens}\")\n",
        "    \n",
        "    if beam_width != 0:\n",
        "        print(f\"Using beam search with width {beam_width}\")\n",
        "        return beam_search(model, x, max_new_tokens, beam_width, repetition_penalty)\n",
        "\n",
        "    for i in range(max_new_tokens):\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Generated {i} tokens\")\n",
        "        \n",
        "        if x.size(1) > model.config.block_size:\n",
        "            idx_cond = x[:, -model.config.block_size:]\n",
        "        else:\n",
        "            idx_cond = x\n",
        "        logits, _ = model(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # Apply repetition penalty\n",
        "        if repetition_penalty != 1.0:\n",
        "          # Get unique tokens in the generated sequence\n",
        "          unique_tokens = torch.unique(x[0])\n",
        "          \n",
        "          # Create a mask for the tokens that have been used\n",
        "          mask = torch.zeros_like(logits[0]).bool()\n",
        "          mask[unique_tokens] = True\n",
        "          \n",
        "          # Apply the penalty only to the used tokens\n",
        "          logits[0, mask] /= repetition_penalty\n",
        "\n",
        "        if greedy:\n",
        "            # Greedy decoding: select the token with the highest probability\n",
        "            idx_next = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
        "        else:\n",
        "            # Existing sampling methods\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            if top_p != 0.0:\n",
        "                idx_next = top_p_sampling(logits, top_p)\n",
        "            else:\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        x = torch.cat((x, idx_next), dim=1)\n",
        "\n",
        "    print(\"Generation complete\")\n",
        "    return x\n",
        "```\n",
        "\n",
        "As we can see - if we *don't* use a `top_p`, `top_k`, or `beam_width` - we simply sample the distribution as-in."
      ],
      "metadata": {
        "id": "KrdbM6PLf3B3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cl_maPrbRbjJ",
        "outputId": "92c5c19b-ab53-4d3e-e38d-e5c67dcce958"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "2024-12-03 22:20:36.999246: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-03 22:20:37.015968: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 22:20:37.036982: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 22:20:37.043291: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 22:20:37.057991: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 22:20:38.216019: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "One possibility is that they have a universe-wide perspective that allows for no contradictions. And, if so, then why is there contradiction?\n",
            "\n",
            "Another possibility is that the universe is one big joke. They are not the only joke in the universe. But it is not an empty universe.\n",
            "\n",
            "Hence the problems with the universe's Big Bang theory, which tells us that the universe started out in a singularity with a zero initial mass. The Big Bang is essentially consistent with\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Greedy Decoding\n",
        "\n",
        "Greedy decoding is the simplest method for generating text from a language model. It works as follows:\n",
        "\n",
        "1. **Prediction**: At each step, the model predicts the probability distribution for the next token.\n",
        "\n",
        "2. **Selection**: The token with the highest probability is selected.\n",
        "\n",
        "3. **Iteration**: This process is repeated until the desired length is reached or a stop condition is met.\n",
        "\n",
        "Key characteristics of greedy decoding:\n",
        "\n",
        "- **Deterministic**: Given the same input and model state, it always produces the same output.\n",
        "- **Fast**: It's computationally efficient as it doesn't require sampling or complex calculations.\n",
        "- **Lack of diversity**: It tends to generate repetitive and sometimes boring text, especially for longer sequences.\n",
        "\n",
        "Let's check out the code!\n",
        "\n",
        "```python\n",
        "if greedy:\n",
        "    # Greedy decoding: select the token with the highest probability\n",
        "    idx_next = torch.argmax(logits, dim=-1).unsqueeze(0)\n",
        "```"
      ],
      "metadata": {
        "id": "aR0f32wMmJ3d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "And see it in practice!"
      ],
      "metadata": {
        "id": "We72GXX1n1Wo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --greedy=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TI8jlgeCSD8Q",
        "outputId": "2bb35f48-7af4-450a-e1ae-56693180c1b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: greedy = True\n",
            "2024-12-03 22:21:23.882512: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-03 22:21:23.900975: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 22:21:23.924482: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 22:21:23.930950: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 22:21:23.946143: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 22:21:25.112333: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "The answer is that we don't know.\n",
            "\n",
            "We don't know what the universe is made of, or how it came to be.\n",
            "\n",
            "We don't know what the laws of nature are.\n",
            "\n",
            "We don't know what the purpose of life is.\n",
            "\n",
            "We don't know what the meaning of life is.\n",
            "\n",
            "We don't know what the purpose of the universe is.\n",
            "\n",
            "We don't know what the purpose of the universe is.\n",
            "\n",
            "\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "That is a bit repetitive, is there way we can reduce this?"
      ],
      "metadata": {
        "id": "a4SjjQNtlOyR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Repetition Penalty\n",
        "\n",
        "Repetition penalty is a technique used in text generation to reduce the likelihood of a model repeating the same words or phrases too frequently. It works by dynamically adjusting the probability of tokens that have already been generated.\n",
        "\n",
        "Key aspects of repetition penalty:\n",
        "\n",
        "1. **Penalization**: It decreases the probability of tokens that have appeared in the generated text.\n",
        "2. **Adjustable strength**: The penalty can be tuned to control the trade-off between coherence and diversity.\n",
        "3. **Applicability**: It can be applied alongside other decoding methods like greedy search, beam search, or sampling techniques.\n",
        "\n",
        "The basic idea is to divide the logits (unnormalized prediction scores) of previously generated tokens by a penalty factor.\n",
        "\n",
        "```python\n",
        "if repetition_penalty != 1.0:\n",
        "    # Consider only the last 1000 tokens (adjust as needed)\n",
        "    window_size = 1000\n",
        "    recent_tokens = x[0, -window_size:]\n",
        "    \n",
        "    # Get unique tokens in the recent sequence\n",
        "    unique_tokens = torch.unique(recent_tokens)\n",
        "    \n",
        "    # Create a mask for the tokens that have been used recently\n",
        "    mask = torch.zeros_like(logits[0]).bool()\n",
        "    mask[unique_tokens] = True\n",
        "    \n",
        "    # Apply the penalty only to the recently used tokens\n",
        "    logits[0, mask] /= repetition_penalty\n",
        "```"
      ],
      "metadata": {
        "id": "3waoOo9Hn5F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see it in practice!"
      ],
      "metadata": {
        "id": "rsRe-kI4oC8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --greedy=True --repetition_penalty=1.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22Xdcf8JlqCp",
        "outputId": "1e143f5f-b3c1-4bd0-f427-97a674e17b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: greedy = True\n",
            "Overriding: repetition_penalty = 1.8\n",
            "2024-12-03 22:22:10.499402: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-03 22:22:10.516611: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 22:22:10.538216: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 22:22:10.544644: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 22:22:10.559961: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 22:22:11.734206: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "The question of what we are made from has been a central one for humanity since our earliest days. The first humans were thought by many people at that time (and still today) as being \"primitive\" or having no souls; they had only bodies with which their minds could communicate through touch alone.[1] However this view was quickly overturned when modern science began its study on human anatomy in earnest during the 19th century,[2][3]. It became clear over these centuries how much\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-P (Nucleus) Sampling\n",
        "\n",
        "Top-P sampling, also known as nucleus sampling, is a text generation technique that provides a balance between diversity and quality in the generated text. It works as follows:\n",
        "\n",
        "1. **Sorting probabilities**: After the model produces the probability distribution for the next token, the probabilities are sorted in descending order.\n",
        "\n",
        "2. **Cumulative sum**: A cumulative sum of these sorted probabilities is calculated.\n",
        "\n",
        "3. **Probability mass selection**: A threshold p (typically between 0.9 and 1) is chosen. The smallest set of tokens whose cumulative probability exceeds p is selected.\n",
        "\n",
        "4. **Sampling**: The next token is randomly sampled from this reduced set of tokens.\n",
        "\n",
        "Key advantages of Top-P sampling:\n",
        "\n",
        "- It adapts to the confidence of the model's predictions.\n",
        "- It can produce more diverse outputs than methods like Top-K sampling, especially for less confident predictions.\n",
        "- It helps avoid low-probability tokens while maintaining a dynamic vocabulary size.\n",
        "\n",
        "```python\n",
        "def top_p_sampling(logits, p):\n",
        "    probs = F.softmax(logits, dim=-1)\n",
        "    sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
        "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "    sorted_indices_to_remove = cumulative_probs > p\n",
        "    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "    sorted_indices_to_remove[..., 0] = 0\n",
        "    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "    probs = probs.masked_fill(indices_to_remove, 0.0)\n",
        "    return torch.multinomial(probs, num_samples=1)\n",
        "```"
      ],
      "metadata": {
        "id": "nsOQSY6jm_yY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see it in action!"
      ],
      "metadata": {
        "id": "sCMuIwdXoXoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --top_p=0.95"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8sfAMjToaCo",
        "outputId": "0b6903a5-4876-4b0d-c74f-8f62b335a6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: top_p = 0.95\n",
            "2024-12-03 22:22:56.996713: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-03 22:22:57.013154: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 22:22:57.033708: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 22:22:57.039936: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 22:22:57.054595: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 22:22:58.223796: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "One possibility is that they have a universe-wide perspective that allows them to think in general.\n",
            "\n",
            "If that's the case, then an individual's life might seem like nothing but one tiny drop in a vast ocean of universes.\n",
            "\n",
            "But if that's not the case, then the answer is still far from clear.\n",
            "\n",
            "So far, the universe is the only place we've ever known, and it has caused us no problem so far.\n",
            "\n",
            "So does that mean\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see this with a low temperature, and a high temperature and observe the difference!"
      ],
      "metadata": {
        "id": "1disLRV_o-AQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cold as ice:"
      ],
      "metadata": {
        "id": "-xKzCGWspG2a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --top_p=0.95 --temperature=0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqmAjRNUpCJ5",
        "outputId": "b475ade1-5351-41a2-a2cb-cbbb4ee5f6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: top_p = 0.95\n",
            "Overriding: temperature = 0.1\n",
            "2024-12-03 22:23:43.741484: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-03 22:23:43.757501: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 22:23:43.777998: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 22:23:43.784226: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 22:23:43.798710: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 22:23:44.957108: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "The answer is that we don't know.\n",
            "\n",
            "We don't know what the universe is made of, or how it came to be.\n",
            "\n",
            "We don't know what the laws of nature are.\n",
            "\n",
            "We don't know what the laws of physics are.\n",
            "\n",
            "We don't know what the laws of chemistry are.\n",
            "\n",
            "We don't know what the laws of biology are.\n",
            "\n",
            "We don't know what the laws of physics are.\n",
            "\n",
            "We don\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Burning hot:"
      ],
      "metadata": {
        "id": "3ZhcKjVgpIVH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=100 --top_p=0.05 --temperature=100.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "884hReuEpFmG",
        "outputId": "a113cb94-7037-48e1-b43d-9ee2836bb7d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 100\n",
            "Overriding: top_p = 0.05\n",
            "Overriding: temperature = 100.0\n",
            "2024-12-03 22:24:30.735687: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-03 22:24:30.752270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 22:24:30.772900: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 22:24:30.779181: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 22:24:30.794060: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 22:24:31.990515: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=100\n",
            "Generated 0 tokens\n",
            "Generation complete\n",
            "What is the answer to life, the universe, and everything? It may come, according to a newly developed theory which could solve all mysteries that physicists are seeking to resolve about why and by why we are here.\"The Big Question, the first ever paper by Nobel prize physicist Steven Jaki on 'Life as You Have Never Kneeled before: An Introduction into the Physics that Drices the Creation, and the Endeavor to Know About the Origin (PDF)\" was submitted by Jakis as he submitted the article and is being posted here on our main page\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search\n",
        "\n",
        "Beam search is a heuristic search algorithm commonly used in natural language processing for text generation tasks. It's an extension of greedy decoding that explores multiple possible sequences simultaneously.\n",
        "\n",
        "Key concepts:\n",
        "\n",
        "1. **Beam width**: The number of sequences to keep track of at each step.\n",
        "2. **Expanding**: At each step, generate all possible next tokens for each sequence in the beam.\n",
        "3. **Pruning**: Keep only the top-k sequences (where k is the beam width) based on their cumulative probability scores.\n",
        "\n",
        "Algorithm overview:\n",
        "\n",
        "1. Start with an initial sequence (usually a start token).\n",
        "2. Generate the next token probabilities for each sequence in the beam.\n",
        "3. Create new candidate sequences by appending each possible next token.\n",
        "4. Score each candidate sequence based on its cumulative log probability.\n",
        "5. Select the top-k sequences to form the new beam.\n",
        "6. Repeat steps 2-5 until the desired length is reached or a stop condition is met."
      ],
      "metadata": {
        "id": "lTp4D2iBoZIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"What is the answer to life, the universe, and everything?\" \\\n",
        "    --num_samples=1 --max_new_tokens=50 --beam_width=16 --repetition_penalty=1.8 --temperature=1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L06prEqhToUj",
        "outputId": "05bcd655-ca6a-4480-cd72-3198ab2335d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = What is the answer to life, the universe, and everything?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 50\n",
            "Overriding: beam_width = 16\n",
            "Overriding: repetition_penalty = 1.8\n",
            "Overriding: temperature = 1.4\n",
            "2024-12-03 22:25:17.733287: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-03 22:25:17.749910: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 22:25:17.770475: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 22:25:17.776758: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 22:25:17.791556: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 22:25:18.931946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=50\n",
            "Using beam search with width 16\n",
            "What is the answer to life, the universe, and everything?\n",
            "\n",
            "I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't know. I don't\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python extended_sample.py \\\n",
        "    --init_from=gpt2-xl \\\n",
        "    --start=\"Why is the sky blue?\" \\\n",
        "    --num_samples=1 --max_new_tokens=50 --beam_width=16 --repetition_penalty=1.8 --temperature=1.4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_-bvuRLvURk",
        "outputId": "894a1ef1-861a-48d8-fc0f-856858efb3c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overriding: init_from = gpt2-xl\n",
            "Overriding: start = Why is the sky blue?\n",
            "Overriding: num_samples = 1\n",
            "Overriding: max_new_tokens = 50\n",
            "Overriding: beam_width = 16\n",
            "Overriding: repetition_penalty = 1.8\n",
            "Overriding: temperature = 1.4\n",
            "2024-12-03 22:26:21.391729: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2024-12-03 22:26:21.408336: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-12-03 22:26:21.429152: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-12-03 22:26:21.435411: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-12-03 22:26:21.450154: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-12-03 22:26:22.602210: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "loading weights from pretrained gpt: gpt2-xl\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.0\n",
            "number of parameters: 1555.97M\n",
            "No meta.pkl found, assuming GPT-2 encodings...\n",
            "Generating sample 1/1\n",
            "Starting generation with max_new_tokens=50\n",
            "Using beam search with width 16\n",
            "Why is the sky blue?\n",
            "\n",
            "The answer to this question depends on what you mean by \"sky\" and what you mean by \"blue\".\n",
            "\n",
            "The word \"sky\" has two different meanings in English.\n",
            "\n",
            "The first meaning of \"sky\" refers to a\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ğŸ‘ªâ“ Discussion Question #2:\n",
        "\n",
        "Which decoding method is *best*? If there is no obvious best, please list the pros/cons/use-cases for each method."
      ],
      "metadata": {
        "id": "OCylr1WluXw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Breakout Room #2:"
      ],
      "metadata": {
        "id": "FQIbF5cr9CA-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Speculative Decoding\n",
        "\n",
        "Speculative decoding is a technique that aims to speed up text generation by using a smaller, faster \"draft\" model to predict multiple tokens at once, which are then verified by a larger, more accurate model. Here's how it works:\n",
        "\n",
        "1. Draft Stage:\n",
        "\n",
        "- A smaller, faster model proposes a sequence of tokens\n",
        "- The draft model can generate these quickly but with lower quality\n",
        "- The number of tokens generated is a tunable parameter (num_draft_tokens)\n",
        "\n",
        "\n",
        "2. Verification Stage:\n",
        "\n",
        "- A larger, more accurate model evaluates the proposed tokens\n",
        "- For each token, it calculates the probability ratio between the draft and verifier models\n",
        "- Tokens are accepted if the ratio meets certain criteria\n",
        "\n",
        "\n",
        "3. Accept/Reject Process:\n",
        "\n",
        "- Accepted tokens are added to the sequence\n",
        "- If a token is rejected, we fall back to the verifier's prediction\n",
        "- The process then continues from the last accepted token\n",
        "\n",
        "\n",
        "**Key advantages**:\n",
        "\n",
        "- Can provide significant speedup (2-3x) over traditional decoding\n",
        "- Maintains quality of the larger model\n",
        "- Particularly effective for long sequences"
      ],
      "metadata": {
        "id": "t_N6JOYIq2oz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import gc\n",
        "\n",
        "class DirectSpeculativeDecoder:\n",
        "    def __init__(self, draft_model_name='gpt2', verifier_model_name='gpt2-medium', num_draft_tokens=4):\n",
        "        \"\"\"Initialize decoder with draft and verifier models\"\"\"\n",
        "        # Clear CUDA cache and garbage collect\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        print(f\"Using device: {self.device}\")\n",
        "\n",
        "        # Set model precision\n",
        "        self.dtype = torch.float16 if self.device == 'cuda' else torch.float32\n",
        "\n",
        "        # Load models and tokenizer with proper memory handling\n",
        "        print(f\"Loading draft model: {draft_model_name}\")\n",
        "        self.draft_model = GPT2LMHeadModel.from_pretrained(\n",
        "            draft_model_name,\n",
        "            torch_dtype=self.dtype,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        self.draft_model.eval().to(self.device)\n",
        "\n",
        "        print(f\"Loading verifier model: {verifier_model_name}\")\n",
        "        self.verifier_model = GPT2LMHeadModel.from_pretrained(\n",
        "            verifier_model_name,\n",
        "            torch_dtype=self.dtype,\n",
        "            low_cpu_mem_usage=True\n",
        "        )\n",
        "        self.verifier_model.eval().to(self.device)\n",
        "\n",
        "        self.tokenizer = GPT2Tokenizer.from_pretrained(draft_model_name)\n",
        "        self.num_draft_tokens = num_draft_tokens\n",
        "\n",
        "        # Add padding token if it doesn't exist\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "    def _safe_model_output(self, model, input_ids):\n",
        "        \"\"\"Safely get model outputs with proper error handling\"\"\"\n",
        "        try:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                outputs = model(input_ids)\n",
        "            return outputs\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error during model inference: {str(e)}\")\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            raise\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, prompt, max_new_tokens, temperature=1.0):\n",
        "        \"\"\"Generate text using speculative decoding\"\"\"\n",
        "        try:\n",
        "            # Encode prompt\n",
        "            input_ids = self.tokenizer.encode(prompt, return_tensors='pt').to(self.device)\n",
        "            initial_length = input_ids.size(1)  # Store initial length\n",
        "            tokens_generated = 0  # Track total new tokens\n",
        "            accepted_tokens = 0  # Track accepted speculative tokens\n",
        "\n",
        "            # Ensure we don't exceed model's context length\n",
        "            max_length = min(self.draft_model.config.max_length, self.verifier_model.config.max_length)\n",
        "\n",
        "            while tokens_generated < max_new_tokens and input_ids.size(1) < max_length:\n",
        "                curr_draft_tokens = min(\n",
        "                    self.num_draft_tokens,\n",
        "                    max_new_tokens - tokens_generated,\n",
        "                    max_length - input_ids.size(1)\n",
        "                )\n",
        "\n",
        "                if curr_draft_tokens <= 0:\n",
        "                    break\n",
        "\n",
        "                # Draft stage: Generate candidate tokens\n",
        "                draft_tokens = []\n",
        "                curr_ids = input_ids.clone()\n",
        "\n",
        "                for _ in range(curr_draft_tokens):\n",
        "                    outputs = self._safe_model_output(self.draft_model, curr_ids)\n",
        "                    logits = outputs.logits[:, -1, :] / temperature\n",
        "                    probs = F.softmax(logits, dim=-1)\n",
        "                    next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "                    draft_tokens.append(next_token.item())\n",
        "                    curr_ids = torch.cat((curr_ids, next_token), dim=1)\n",
        "\n",
        "                # Create draft sequence\n",
        "                draft_tensor = torch.tensor([draft_tokens], device=self.device)\n",
        "                draft_sequence = torch.cat((input_ids, draft_tensor), dim=1)\n",
        "\n",
        "                # Verify stage\n",
        "                verifier_outputs = self._safe_model_output(self.verifier_model, draft_sequence)\n",
        "                verifier_logits = verifier_outputs.logits[:, input_ids.size(1)-1:input_ids.size(1)+curr_draft_tokens-1, :] / temperature\n",
        "                verifier_probs = F.softmax(verifier_logits, dim=-1)\n",
        "\n",
        "                # Accept/reject tokens\n",
        "                accepted_sequence = input_ids.clone()\n",
        "                tokens_accepted_this_round = 0\n",
        "\n",
        "                for i in range(curr_draft_tokens):\n",
        "                    draft_token = draft_tokens[i]\n",
        "                    verifier_prob = verifier_probs[0, i, draft_token].item()\n",
        "\n",
        "                    if torch.rand(1).item() < verifier_prob:\n",
        "                        accepted_sequence = torch.cat((\n",
        "                            accepted_sequence,\n",
        "                            torch.tensor([[draft_token]], device=self.device)\n",
        "                        ), dim=1)\n",
        "                        accepted_tokens += 1\n",
        "                        tokens_accepted_this_round += 1\n",
        "                    else:\n",
        "                        # Sample new token from verifier if rejected\n",
        "                        new_token = torch.multinomial(verifier_probs[0, i], num_samples=1)\n",
        "                        accepted_sequence = torch.cat((\n",
        "                            accepted_sequence,\n",
        "                            new_token.unsqueeze(0)\n",
        "                        ), dim=1)\n",
        "                        tokens_accepted_this_round += 1\n",
        "                        break\n",
        "\n",
        "                # Update sequence and token counts\n",
        "                input_ids = accepted_sequence\n",
        "                tokens_generated += tokens_accepted_this_round\n",
        "\n",
        "                # Print intermediate result\n",
        "                if tokens_generated % 10 == 0 and tokens_generated > 0:\n",
        "                    print(\"\\nCurrent generation:\", self.tokenizer.decode(input_ids[0]))\n",
        "\n",
        "                # Clear some memory\n",
        "                if self.device == 'cuda':\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            print(f\"\\nAccepted {accepted_tokens}/{tokens_generated} speculative tokens\")\n",
        "            return self.tokenizer.decode(input_ids[0])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during generation: {str(e)}\")\n",
        "            if self.device == 'cuda':\n",
        "                torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            raise"
      ],
      "metadata": {
        "id": "GC7kCovZ1zTe"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we just need some code to actually run that beast!"
      ],
      "metadata": {
        "id": "-_IFUoig7Pdw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_speculative_decoder(prompt, max_tokens=50):\n",
        "    try:\n",
        "        decoder = DirectSpeculativeDecoder(\n",
        "            draft_model_name='gpt2',\n",
        "            verifier_model_name='gpt2-medium',\n",
        "            num_draft_tokens=4\n",
        "        )\n",
        "\n",
        "        print(\"\\nGenerating with prompt:\", prompt)\n",
        "        result = decoder.generate(\n",
        "            prompt=prompt,\n",
        "            max_new_tokens=max_tokens,\n",
        "            temperature=0.8\n",
        "        )\n",
        "        print(\"\\nFinal result:\", result)\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in speculative decoding: {str(e)}\")\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        raise"
      ],
      "metadata": {
        "id": "dEb6JWt66twu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can try it out!"
      ],
      "metadata": {
        "id": "rVA7n6om7SKH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What is the answer to life, the universe, and everything?\"\n",
        "run_speculative_decoder(prompt, max_tokens=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 226
        },
        "id": "XBQh62Vp2Y8L",
        "outputId": "c7867819-cbe3-4b3c-c324-4e64d4129966"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading draft model: gpt2\n",
            "Loading verifier model: gpt2-medium\n",
            "\n",
            "Generating with prompt: What is the answer to life, the universe, and everything?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-444293ac97eb>:47: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Accepted 3/7 speculative tokens\n",
            "\n",
            "Final result: What is the answer to life, the universe, and everything? How can we eliminate the need for\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is the answer to life, the universe, and everything? How can we eliminate the need for'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### â“ Question #2:\n",
        "\n",
        "How does this method offer a speed-up over traditional decoding?"
      ],
      "metadata": {
        "id": "-U5tkFXxr_2p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6: Guard Rails\n",
        "\n",
        "Guard Rails is a technique for controlling the output of language models by filtering or constraining the tokens that can be generated. This provides a way to ensure outputs meet certain criteria without having to retrain or fine-tune the model.\n",
        "\n",
        "\n",
        "Key components:\n",
        "\n",
        "1. Token-Level Filtering:\n",
        "\n",
        "- Maintains a set of blocked token IDs\n",
        "- Applies filtering during the generation process\n",
        "- Can handle both individual tokens and sequences\n",
        "\n",
        "\n",
        "2. Vocabulary Management:\n",
        "\n",
        "- Converts blocked words to token IDs\n",
        "- Handles different forms of words (prefixes, suffixes)\n",
        "- Works with the model's tokenizer\n",
        "\n",
        "\n",
        "3. Probability Modification:\n",
        "\n",
        "- Sets probabilities of blocked tokens to -inf\n",
        "- Preserves relative probabilities of allowed tokens\n",
        "- Integrates with temperature and top-k sampling"
      ],
      "metadata": {
        "id": "WJU15zam1ajP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class TokenGuardRails:\n",
        "    def __init__(self, tokenizer, blocked_token_ids=None):\n",
        "        \"\"\"\n",
        "        Initialize guard rails with blocked token IDs\n",
        "\n",
        "        Args:\n",
        "            tokenizer: The tokenizer to use for converting between tokens and text\n",
        "            blocked_token_ids (set): Set of token IDs that should be blocked\n",
        "        \"\"\"\n",
        "        self.tokenizer = tokenizer\n",
        "        self.blocked_token_ids = set(blocked_token_ids or [])\n",
        "\n",
        "    def filter_logits(self, logits):\n",
        "        \"\"\"\n",
        "        Filter logits by setting probabilities of blocked tokens to -inf\n",
        "\n",
        "        Args:\n",
        "            logits (torch.Tensor): Raw logits from model [batch_size, vocab_size]\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Filtered logits with same shape\n",
        "        \"\"\"\n",
        "        if not self.blocked_token_ids:\n",
        "            return logits\n",
        "\n",
        "        # Create mask for blocked tokens\n",
        "        mask = torch.ones_like(logits, dtype=torch.bool)\n",
        "        mask[:, list(self.blocked_token_ids)] = False\n",
        "\n",
        "        # Set blocked token probabilities to -inf\n",
        "        logits.masked_fill_(~mask, float('-inf'))\n",
        "        return logits\n",
        "\n",
        "@torch.no_grad()\n",
        "def generate_with_token_guard_rails(model, x, guard_rails, max_new_tokens, temperature=1.0, top_k=None):\n",
        "    \"\"\"\n",
        "    Generate text using token-level guard rails\n",
        "\n",
        "    Args:\n",
        "        model: The language model\n",
        "        x (torch.Tensor): Input token IDs [batch_size, seq_len]\n",
        "        guard_rails (TokenGuardRails): Guard rails instance\n",
        "        max_new_tokens (int): Maximum number of tokens to generate\n",
        "        temperature (float): Sampling temperature\n",
        "        top_k (int): If set, use top-k filtering\n",
        "    \"\"\"\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Crop sequence if needed\n",
        "        if x.size(1) > model.config.n_ctx:\n",
        "            idx_cond = x[:, -model.config.n_ctx:]\n",
        "        else:\n",
        "            idx_cond = x\n",
        "\n",
        "        # Get logits from model\n",
        "        logits = model(idx_cond).logits\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        # Apply guard rails filtering\n",
        "        logits = guard_rails.filter_logits(logits)\n",
        "\n",
        "        # Optional top-k filtering\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        # Get probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "        # Sample next token\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Append sampled token\n",
        "        x = torch.cat((x, idx_next), dim=1)\n",
        "\n",
        "    return x\n",
        "\n",
        "def setup_token_guard_rails(tokenizer, blocked_words):\n",
        "    \"\"\"Helper to set up token guard rails from word list\"\"\"\n",
        "    blocked_token_ids = set()\n",
        "\n",
        "    # Get token IDs for each blocked word\n",
        "    for word in blocked_words:\n",
        "        tokens = tokenizer.encode(word, add_special_tokens=False)\n",
        "        blocked_token_ids.update(tokens)\n",
        "\n",
        "        # Also get token IDs for word with different prefixes/suffixes\n",
        "        for prefix in [' ', '\\n', '\\t']:\n",
        "            tokens = tokenizer.encode(prefix + word, add_special_tokens=False)\n",
        "            if len(tokens) > 1:  # Only add if it creates a new token\n",
        "                blocked_token_ids.add(tokens[1])\n",
        "\n",
        "    return TokenGuardRails(tokenizer, blocked_token_ids)\n",
        "\n",
        "def run_guarded_generation(prompt, model, tokenizer, blocked_words, max_new_tokens=100):\n",
        "    # Set up guard rails\n",
        "    guard_rails = setup_token_guard_rails(tokenizer, blocked_words)\n",
        "\n",
        "    # Encode prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(model.device)\n",
        "\n",
        "    # Generate with guard rails\n",
        "    output_ids = generate_with_token_guard_rails(\n",
        "        model=model,\n",
        "        x=input_ids,\n",
        "        guard_rails=guard_rails,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        temperature=0.8,\n",
        "        top_k=40\n",
        "    )\n",
        "\n",
        "    return tokenizer.decode(output_ids[0])"
      ],
      "metadata": {
        "id": "cfmM5TxL5Cov"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Key advantages:**\n",
        "\n",
        "- Fine-grained control over model outputs\n",
        "- No need for model retraining\n",
        "- Can be updated dynamically\n",
        "- Maintains coherence of generated text"
      ],
      "metadata": {
        "id": "UDA0j74x7jYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Initialize model and tokenizer\n",
        "model_name = \"gpt2-medium\"  # Using smaller model for demonstration\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Move to GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = model.to(device)\n",
        "\n",
        "# Test generation with guard rails\n",
        "blocked_words = [\"war\", \"conflict\", \"fight\", \"battle\"]\n",
        "prompt = \"The future of international relations will be characterized by\"\n",
        "\n",
        "print(\"Generating with blocked words:\", blocked_words)\n",
        "print(\"\\nPrompt:\", prompt)\n",
        "print(\"\\nGenerating...\")\n",
        "\n",
        "result = run_guarded_generation(\n",
        "    prompt=prompt,\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    blocked_words=blocked_words,\n",
        "    max_new_tokens=50\n",
        ")\n",
        "\n",
        "print(\"\\nGenerated text:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAs9C2TS5Fju",
        "outputId": "e16fb6cc-c9d9-48e2-d0c3-0680b0190549"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating with blocked words: ['war', 'conflict', 'fight', 'battle']\n",
            "\n",
            "Prompt: The future of international relations will be characterized by\n",
            "\n",
            "Generating...\n",
            "\n",
            "Generated text: The future of international relations will be characterized by a dynamic exchange of ideas and opinions between nations.\"\n",
            "\n",
            "But not all politicians are eager to engage their own countries in this vital task.\n",
            "\n",
            "The Czech Republic and Russia, for instance, have been engaged in a diplomatic war over how to deal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ğŸ‘ªâ“ Question #4:\n",
        "\n",
        "What other applications could Guard Rails have (specifically output Guard Rails)?"
      ],
      "metadata": {
        "id": "NwfYpiibxVeB"
      }
    }
  ]
}