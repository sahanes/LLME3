<p align = "center" draggable=â€falseâ€ ><img src="https://github.com/AI-Maker-Space/LLM-Dev-101/assets/37101144/d1343317-fa2f-41e1-8af1-1dbb18399719" 
     width="200px"
     height="auto"/>
</p>


<h1 align="center" id="heading">:wave: Welcome to: LLM Engineering - Foundations to SLMs!</h1>

## Understand LLMs from first principles; build custom Small Language Models (SLMs)

ğŸ§‘â€ğŸ’» *Language Model Engineering* refers to the evolving set of best practices for **training, fine-tuning**, and **aligning** LLMs to optimize their use and function to balance performance with efficiency.

These best practices have formed the basis for the LLMs, or Large Language Models (a.k.a. Foundation Models) and Small Language Models (SLMs) of today.

ğŸ¤– Whether youâ€™re looking at OpenAIâ€™s GPT series, Anthropic Claude, Grok, Google Gemini, Mistral, or any other model provider, the core underlying transformer architectures are similar, as are the training and tuning methods.

When chasing after high scores on LLM benchmarks or creating state-of-the-art SLMs, techniques like **Model Merging** and **Distillation** are important as well.

ğŸ« This course will provide you with the foundational concepts and code to train, fine-tune, and align state-of-the-art SLMs and LLMs using industry-standard and emerging approaches from the open-source edge heading into 2025.

ğŸ¤“ **Become the expert** in your organization on all things training (pretraining, post-training), fine-tuning (supervised fine-tuning, instruction tuning, chat tuning, etc.), alignment (PPO, DPO, etc.), Small Language Models (SLMs), Model Merging, Distillation, and more!

## By the end of this course, you will be able to:

- ğŸ¦¾ Build, train, and evaluate Language Models using transformer variants (GPT, BERT, BART)
- ğŸ§ Calculate self-attention and understand the latest implementations (Flash Attention, FA2)
- ğŸ”  Demystify embedding layers, embedding representations, pre-trained vs. learned embeddings, ROPE, and more!
- ğŸª™ Decode embedding space representations for optimal next-token prediction
- ğŸ”¡ Build, train, and evaluate embedding models (like those in ğŸ¤— Sentence Transformers)
- ğŸš‡ Complete unsupervised and continued pretraining of LLMs and SLMs from scratch
- ğŸš‰ Fine-tune pre-trained LMs for instruction-following, chat, and more via parameter-efficient methods
- ğŸ›¤ï¸ Align LMs to balance helpfulness with harmlessness and other criteria (RLXF, DPO)
- ğŸš€ Explore frontiers of Language Models (Mixture-of- approaches, Model Merging, alternative fine-tuning, and more!)

## ğŸ™ Contributions

We believe in the power of collaboration. Contributions, ideas, and feedback are highly encouraged! Let's build the ultimate resource for LLMEs together. ğŸ¤

Feel free to reach out with any questions or suggestions. Happy coding! ğŸš€ğŸ”®

ğŸ‘¤ Follow us on [Twitter](https://twitter.com/AIMakerspace) and [LinkedIn](https://www.linkedin.com/company/ai-maker-space) for the latest news!
